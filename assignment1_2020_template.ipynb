{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2020 Semester 1\n",
    "\n",
    "## Assignment 1: Naive Bayes Classifiers\n",
    "\n",
    "###### Submission deadline: 7 pm, Monday 20 Apr 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name(s):**    Aneesh Chattaraj\n",
    "\n",
    "**Student ID(s):**     826860\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(filename, classcolumn,id_code):\n",
    "    temp = pd.read_csv(filename, skiprows=0, header=None, delimiter=';', skip_blank_lines=True)\n",
    "    df=temp[0].str.split(',',expand=True)\n",
    "    class1= df[classcolumn-1]\n",
    "    df.drop(labels=[classcolumn-1], axis=1,inplace = True)\n",
    "    df.insert(len(df.columns), 'class', class1)\n",
    "    if id_code != 'none':\n",
    "        del df[id_code]\n",
    "    data=df.values.tolist()\n",
    "    \n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate prior\n",
    "def prior_func(data):\n",
    "    prob={}\n",
    "    for row in data: \n",
    "        if row[-1] in prob:\n",
    "            prob[row[-1]]+=1\n",
    "        else:\n",
    "            prob[row[-1]]=1\n",
    "    for row in prob.keys():\n",
    "        prob[row]=prob[row]/len(data)\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate the prior probabilities\n",
    "\n",
    "def posterior_func(data):\n",
    "    prob = defaultdict(lambda : defaultdict(lambda : defaultdict(int)))\n",
    "    \n",
    "    for row in data:\n",
    "        attr= row[:-1]\n",
    "        classes= row[-1]\n",
    "        for attr, a in enumerate(attr):\n",
    "            prob[classes][attr][a] += 1\n",
    "    for classes in prob:\n",
    "        for attr in prob[classes]:\n",
    "            l = sum(prob[classes][attr].values())\n",
    "            for a in prob[classes][attr]:\n",
    "                 prob[classes][attr][a] =  prob[classes][attr][a]/l\n",
    "    \n",
    "    \n",
    "    prob=dict(prob)\n",
    "    for classes in prob:\n",
    "        prob[classes]=dict(prob[classes])\n",
    "        for attr in prob[classes]:\n",
    "             prob[classes][attr]=dict( prob[classes][attr])\n",
    "   \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train NB and get prior and posterior\n",
    "\n",
    "def train(data):\n",
    "    return (prior_func(data),posterior_func(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in a test dataset (for the purposes of this assignment, you\n",
    "# can re-use the training data as a test set)\n",
    "def predict(preprocessed_data, trained_set):\n",
    "   \n",
    "    prior_prob, post_prob= trained_set[0], trained_set[1]\n",
    "    length= len(preprocessed_data)\n",
    "    predicted_class=[]\n",
    "    for row in preprocessed_data:\n",
    "        prediction ={}\n",
    "        for a, b in prior_prob.items():\n",
    "            prediction[a]=b\n",
    "            for i in range(len(row)-1):\n",
    "                if row[i] in post_prob[a][i]:\n",
    "                    prediction[a] =prediction[a]* post_prob[a][i][row[i]]\n",
    "                else:\n",
    "                    prediction[a]= prediction[a]*0.01\n",
    "                    prediction[a]= prediction[a]/length\n",
    "        max_key = max(prediction, key=prediction.get)\n",
    "        predicted_class.append(max_key)\n",
    "    \n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "def evaluate(filename, classcolumn,id_code):\n",
    "    preprocessed_data=preprocess(filename, classcolumn,id_code)\n",
    "    trained_set=train(preprocessed_data)\n",
    "    total=0\n",
    "    predict1=predict(preprocessed_data, trained_set)\n",
    "    count=len(preprocessed_data)\n",
    "    \n",
    "    for a in range(count):\n",
    "        if  preprocessed_data[a][-1]== predict1[a]:\n",
    "            total=total+1\n",
    "    \n",
    "    \n",
    "    accuracy= total/count\n",
    "    \n",
    "    \n",
    "    return accuracy*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast-cancer-wisconsin.data      97.56795422031473\n",
      "mushroom.data      99.1506646971935\n",
      "lymphography.data      89.1891891891892\n",
      "wine.data      100.0\n",
      "car.data      87.38425925925925\n",
      "nursery.data      90.30864197530865\n",
      "somerville.data      67.13286713286713\n",
      "adult.data      93.78704585240011\n",
      "bank.data      92.33814779589038\n",
      "university.data      93.10344827586206\n",
      "wdbc.data      100.0\n"
     ]
    }
   ],
   "source": [
    "list_datasets=[[\"breast-cancer-wisconsin.data\",11,0],[\"mushroom.data\",1,\"none\"],[\"lymphography.data\",1,\"none\"],\n",
    "               [\"wine.data\",1,\"none\"],[\"car.data\",7,\"none\"],[\"nursery.data\",9,\"none\"],[\"somerville.data\",1,\"none\"],\n",
    "               [\"adult.data\",15,\"none\"],[\"bank.data\",15,\"none\"],[\"university.data\",14,0],[\"wdbc.data\",2,0]]\n",
    "list_datasets_og=[]\n",
    "for a in range(len(list_datasets)):\n",
    "    b=evaluate(list_datasets[a][0],list_datasets[a][1],list_datasets[a][2])\n",
    "    list_datasets_og.append(b)\n",
    "    print(str(list_datasets[a][0])+'      '+str(b))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to question (1), and **one** other of your choosing (two responses in total).\n",
    "\n",
    "If you are in a group of 2, you will respond to question (1) and question (2), and **two** others of your choosing (four responses in total). \n",
    "\n",
    "A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions in respond to the question, but your formal answer should be added to a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Try discretising the numeric attributes in these datasets and treating them as discrete variables in the na¨ıve Bayes classifier. You can use a discretisation method of your choice and group the numeric values into any number of levels (but around 3 to 5 levels would probably be a good starting point). Does discretizing the variables improve classification performance, compared to the Gaussian na¨ıve Bayes approach? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#binning with 3 to 5 levels and discretising the numeric data\n",
    "from pandas import DataFrame\n",
    "import math\n",
    "import pandas\n",
    "df=preprocess(\"wdbc.data\",2,0)\n",
    "df= DataFrame(df)\n",
    "\n",
    "classes= df[30]\n",
    "for i in range(30):\n",
    "    k=0\n",
    "    k=df[i].astype(float).max()\n",
    "    f=df[i].astype(float).min()\n",
    "    \n",
    "    \n",
    "    for j in range(569):\n",
    "        df[i][j]= math.floor(math.ceil(float(df[i][j]) * 5) / k)\n",
    "\n",
    "df.to_csv(r'address of folder to save the file as wdbc_new.data', index = False)\n",
    "\n",
    "evaluate('wdbc_new.data',30,\"none\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.64804469273743"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=preprocess(\"wine.data\",1,\"none\")\n",
    "df= DataFrame(df)\n",
    "\n",
    "classes= df[0]\n",
    "for i in range(13):\n",
    "    k=0\n",
    "    k=df[i].astype(float).max()\n",
    "    for j in range(178):\n",
    "        df[i][j]= math.floor(math.ceil(float(df[i][j]) * 3) / k)\n",
    "\n",
    "df.to_csv(r'address of folder to save the file as wine_new.data', index = False)\n",
    "evaluate('wine_new.data',14,\"none\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for wine.data 98.87640449438202 %\n",
      "Accuracy for wdbc 94.20035149384886 %\n"
     ]
    }
   ],
   "source": [
    "df=preprocess(\"wine.data\",1,\"none\")\n",
    "df= DataFrame(df)\n",
    "classes= df[13]\n",
    "del df[13]\n",
    "train=df\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "GNB_model=model.fit(train,classes)\n",
    "pred = GNB_model.predict(train)\n",
    "\n",
    "accuracy = accuracy_score(classes, pred)\n",
    "print('Accuracy for wine.data',100*accuracy,'%')\n",
    "\n",
    "\n",
    "df=preprocess(\"wdbc.data\",2,0)\n",
    "df= DataFrame(df)\n",
    "classes= df[30]\n",
    "del df[30]\n",
    "train=df\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "GNB_model=model.fit(train,classes)\n",
    "pred = GNB_model.predict(train)\n",
    "\n",
    "accuracy = accuracy_score(classes, pred)\n",
    "print('Accuracy for wdbc',100*accuracy,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer Q1: Naive Bayes treats all features as being independent of one another. It works best with categorical datasets and smaller datasets. Discretisation is helful when dealing with numerical datasets to convert them into values depending on the number of bins selected and maximum and minimum value of the entire column. From our results we can see our Naive Bayes has 100% accuracy on both of our numerical datasets, a strange observation. When we discretisize the datasets into bins of 5 we can still see that wdbc.data having 28 columns x 569 rows still got 100% accuracy this could be due to grouping ofnumerical data over a small bin size producing similar results. While wine.data had a reduced accuracy could be because of the number of classes to identify. They slightly perform better than gaussian Naive bayes depending upon the levels or bin size taken. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Implement a baseline model (e.g., random or 0R) and compare the performance of the na¨ıve Bayes classifier to this baseline on multiple datasets. Discuss why the baseline performance varies across datasets, and to what extent the na¨ıve Bayes classifier improves on the baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast-cancer-wisconsin.data      0.6552217453505007   vs  97.56795422031473\n",
      "-32.84457478005865\n",
      "mushroom.data      0.517971442639094   vs  99.1506646971935\n",
      "-47.75915580384854\n",
      "lymphography.data      0.5472972972972973   vs  89.1891891891892\n",
      "-38.63636363636364\n",
      "wine.data      0.398876404494382   vs  100.0\n",
      "-60.1123595505618\n",
      "car.data      0.7002314814814815   vs  87.38425925925925\n",
      "-19.867549668874158\n",
      "nursery.data      0.3333333333333333   vs  90.30864197530865\n",
      "-63.08954203691046\n",
      "somerville.data      0.5384615384615384   vs  67.13286713286713\n",
      "-19.791666666666664\n",
      "adult.data      0.7591904425539756   vs  93.78704585240011\n",
      "-19.051673325037655\n",
      "bank.data      0.8830151954170445   vs  92.33814779589038\n",
      "-4.371571609936043\n",
      "university.data      0.3620689655172414   vs  93.10344827586206\n",
      "-61.111111111111114\n",
      "wdbc.data      0.6274165202108963   vs  100.0\n",
      "-37.258347978910365\n",
      "average performance comparison-36.71762874257083\n"
     ]
    }
   ],
   "source": [
    "def zero_rule_algorithm_classification(train, test):\n",
    "    output_values = [row[-1] for row in train]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    predicted = [prediction for i in range(len(test))]\n",
    "    return predicted\n",
    "list_datasets=[[\"breast-cancer-wisconsin.data\",11,0],[\"mushroom.data\",1,\"none\"],[\"lymphography.data\",1,\"none\"],\n",
    "               [\"wine.data\",1,\"none\"],[\"car.data\",7,\"none\"],[\"nursery.data\",9,\"none\"],[\"somerville.data\",1,\"none\"],\n",
    "               [\"adult.data\",15,\"none\"],[\"bank.data\",15,\"none\"],[\"university.data\",14,0],[\"wdbc.data\",2,0]]\n",
    "l=[]\n",
    "sum=0\n",
    "for a in range(len(list_datasets)):\n",
    "    df=preprocess(list_datasets[a][0],list_datasets[a][1],list_datasets[a][2])\n",
    "    total=0\n",
    "    k=zero_rule_algorithm_classification(df,df)\n",
    "    output_values = [row[-1] for row in df]\n",
    "    for b in range(len(df)):\n",
    "        if  k[b]== output_values[b]:\n",
    "            total=total+1\n",
    "        accuracy= total/len(df)\n",
    "    print(str(list_datasets[a][0])+'      '+str(accuracy)+'   vs  '+str(list_datasets_og[a]))\n",
    "    l.append(((accuracy*100)-list_datasets_og[a])/list_datasets_og[a])\n",
    "    print(l[a]*100)\n",
    "    sum+=l[a]*100\n",
    "print('average performance comparison'+str(sum/11))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER Q2: Comparing Naive Bayes with 0R classifier it performs much better when dealing with most of the datasets provided. From the results we found that Naive Bayes performs better by an average of 36.71% compared to a base classifier 0R. It performs worse comparatively as the number of instances increases or the number of instances and attributes are just low enough for naive bayes to get good results irrespective of the type of data. Considering numerical datasets wine and wdbc even though wdbc has more attributes and instances it performs better on that compared to wine, maybe because of the number of classes to predict since wdbc has only 2 vs wine having 3. Again this could be seen in bank.data having one of the largest datasets with only two classes to predict. Considering the ordinal datasets it performs worse on nursery having 5 classes to predict compared to car.data and somerville.data again could be inference on the number of classes and then coming in second the size of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Since it’s difficult to model the probabilities of ordinal data, ordinal attributes are often treated as either nominal variables or numeric variables. Compare these strategies on the ordinal datasets provided. Deterimine which approach gives higher classification accuracy and discuss why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy (you should implement this yourself and do not simply call existing implementations from `scikit-learn`). How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the na¨ıve Bayes classifier? Explain why, or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "The Gaussian na¨ıve Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in these datasets? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the NB classifier’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
